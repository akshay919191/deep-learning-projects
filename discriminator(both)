import pandas as pd 
from tensorflow.keras import layers , models
import tensorflow as tf

latent_dim = 100     #dimension of random noise
img_size = 64
chnnel = 3      #3 chnnel rgb
learn_rate_g = 2e-4
learn_rate_d = 2e-4
batch_size = 128    #batch size for data size for model each epoch

def build_discriminator(): # clasification tool used to predict in probability that image is real or fake
    model = tf.keras.Sequential()  #using sequential to initialise layering

    model.add(layers.InputLayer(input_shape = (img_size , img_size , chnnel))) #its a initializing channel which show how the data features are 

    model.add(layers.Conv2D(64 , (5 , 5) , strides = (2 , 2) , padding = 'same')) #initializing layer 
    model.add(layers.LeakyReLU(negative_slope=0.2))  # -ve value got decrease and made +ve
    model.add(layers.Dropout(0.3))  # dropping 30% features 

    model.add(layers.Conv2D(128 , (5 , 5) , strides = (2 , 2) , padding = 'same'))
    model.add(layers.LeakyReLU(negative_slope=0.2))
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())  # converting to 1D 

    model.add(layers.Dense(1 , activation = 'sigmoid')) # predicting tool

    return model

discriminator = build_discriminator()
discriminator.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate_d, beta_1=0.5),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=['accuracy']
)    # compiling features


def build_generator():#tool for making fake image using noise's
    model = tf.keras.Sequential()

    model.add(layers.Dense(4 * 4 * 256 , input_shape = (latent_dim)))  # start deconvolutional neuraling
    model.add(layers.BatchNormalization()) # normalizing into short range of numbers
    model.add(layers.LeakyReLU())
    model.add(layers.Reshape((4 , 4 , 256))) # define the target shape of the data




    #initializing 3 layers of conv2d of transpose
    model.add(layers.Conv2DTranspose(128 , (5 , 5) , strides = (2 , 2) , padding = 'same') , activation='tanh')
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64 , (5 , 5) , strides = (2 , 2) , padding = 'same') , activation='tanh')
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(32 , (5 , 5) , strides = (2 , 2) , padding = 'same') , activation='tanh')
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(chnnel , (5 , 5) , strides = (2 , 2) , padding = 'same') , activation='tanh')

    return model


generator = build_generator()


discriminator.trainable = False
gan_model = models.Sequential([generator , discriminator]) # making gan use both so we can make a image from noise and got to know if its real or fake

gan_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate_g, beta_1=0.5),
    loss=tf.keras.losses.BinaryCrossentropy()
)
